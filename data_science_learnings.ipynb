{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Learnings - Kaggle\n",
    "\n",
    "A personal notebook covering the core concepts learned through Kaggle's Data Science courses.\n",
    "\n",
    "**Topics covered:**\n",
    "1. Pandas — reading data, Series, DataFrames, `describe()`, `value_counts()`\n",
    "2. Scikit-learn — Decision Trees (with `max_leaf_nodes`), Random Forests\n",
    "3. Model Evaluation — Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Pandas\n",
    "\n",
    "Pandas is the core library for loading and manipulating tabular data in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reading Data\n",
    "\n",
    "The most common read function is `pd.read_csv()`. Pandas also supports Excel, JSON, SQL, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The most common way to load data:\n",
    "# df = pd.read_csv('path/to/file.csv')\n",
    "\n",
    "# Other read functions:\n",
    "# pd.read_excel('data.xlsx')      -> Excel files\n",
    "# pd.read_json('data.json')       -> JSON files\n",
    "# pd.read_sql(query, connection)  -> SQL databases\n",
    "# pd.read_parquet('data.parquet') -> Parquet files (efficient columnar format)\n",
    "\n",
    "# For this notebook i'll generate a dataset using sklearn so it's self-contained\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Series\n",
    "\n",
    "A **Series** is a single column of data — essentially a labeled one-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a single column returns a Series\n",
    "house_age = df['HouseAge']\n",
    "\n",
    "print(type(house_age))   # <class 'pandas.core.series.Series'>\n",
    "print()\n",
    "print(house_age.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also create a Series manually\n",
    "manual_series = pd.Series([10, 20, 30, 40, 50], name='example')\n",
    "print(manual_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 DataFrames\n",
    "\n",
    "A **DataFrame** is a table — a collection of Series sharing the same index. Think of it as a spreadsheet in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df))      # <class 'pandas.core.frame.DataFrame'>\n",
    "print('Shape:', df.shape)   # (rows, columns)\n",
    "print('Columns:', df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting multiple columns returns a DataFrame (not a Series)\n",
    "subset = df[['HouseAge', 'AveRooms', 'MedHouseVal']]\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful DataFrame inspection methods\n",
    "df.info()    # column names, non-null counts, dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 `describe()`\n",
    "\n",
    "`describe()` gives a quick statistical summary of all numeric columns: count, mean, std, min, quartiles, and max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also call it on a single column (Series)\n",
    "df['MedHouseVal'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 `value_counts()`\n",
    "\n",
    "`value_counts()` counts how many times each unique value appears in a Series. Very useful for categorical or discrete columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HouseAge is discrete (in years), so value_counts is useful here\n",
    "df['HouseAge'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize=True gives proportions instead of raw counts\n",
    "df['HouseAge'].value_counts(normalize=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Prediction Models with Scikit-learn\n",
    "\n",
    "The standard workflow in sklearn:\n",
    "1. Define features (`X`) and target (`y`)\n",
    "2. Split data into train/validation sets\n",
    "3. Instantiate and fit the model\n",
    "4. Make predictions\n",
    "5. Evaluate with MAE (Mean Absolute Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['MedHouseVal']   # target: median house value\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Training rows  :', len(X_train))\n",
    "print('Validation rows:', len(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Decision Tree\n",
    "\n",
    "A Decision Tree splits data into branches based on feature values, arriving at a prediction at each leaf.\n",
    "\n",
    "- **Overfitting**: a deep tree memorises the training data but performs poorly on new data.\n",
    "- **Underfitting**: a shallow tree is too simple to capture patterns.\n",
    "- `max_leaf_nodes` controls the maximum number of leaves — it's the main knob for tuning this tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Default tree (no limit — will overfit)\n",
    "dt_default = DecisionTreeRegressor(random_state=42)\n",
    "dt_default.fit(X_train, y_train)\n",
    "\n",
    "preds_default = dt_default.predict(X_val)\n",
    "mae_default = mean_absolute_error(y_val, preds_default)\n",
    "print(f'Decision Tree (default) — Validation MAE: {mae_default:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning `max_leaf_nodes`\n",
    "\n",
    "We can try different values and pick the one that gives the lowest validation MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(max_leaf_nodes, X_train, X_val, y_train, y_val):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    return mean_absolute_error(y_val, preds)\n",
    "\n",
    "leaf_counts = [5, 10, 25, 50, 100, 250, 500, 1000]\n",
    "\n",
    "results = {n: get_mae(n, X_train, X_val, y_train, y_val) for n in leaf_counts}\n",
    "\n",
    "print(f\"{'max_leaf_nodes':>16} | {'MAE':>8}\")\n",
    "print('-' * 28)\n",
    "for nodes, mae in results.items():\n",
    "    print(f\"{nodes:>16} | {mae:>8.4f}\")\n",
    "\n",
    "best_leaf_nodes = min(results, key=results.get)\n",
    "print(f\"\\nBest max_leaf_nodes: {best_leaf_nodes}  (MAE = {results[best_leaf_nodes]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final Decision Tree with the best max_leaf_nodes\n",
    "dt_best = DecisionTreeRegressor(max_leaf_nodes=best_leaf_nodes, random_state=42)\n",
    "dt_best.fit(X_train, y_train)\n",
    "\n",
    "mae_best_dt = mean_absolute_error(y_val, dt_best.predict(X_val))\n",
    "print(f'Decision Tree (max_leaf_nodes={best_leaf_nodes}) — Validation MAE: {mae_best_dt:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest\n",
    "\n",
    "A Random Forest builds **many** decision trees on random subsets of the data and features, then **averages** their predictions.\n",
    "\n",
    "This reduces overfitting without requiring careful tuning of `max_leaf_nodes`. It almost always outperforms a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_preds = rf_model.predict(X_val)\n",
    "mae_rf = mean_absolute_error(y_val, rf_preds)\n",
    "print(f'Random Forest (100 trees) — Validation MAE: {mae_rf:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'Decision Tree (default / unlimited)': mae_default,\n",
    "    f'Decision Tree (max_leaf_nodes={best_leaf_nodes})': mae_best_dt,\n",
    "    'Random Forest (100 trees)': mae_rf,\n",
    "}\n",
    "\n",
    "print(f\"{'Model':<45} | {'Validation MAE':>14}\")\n",
    "print('-' * 63)\n",
    "for model_name, mae in summary.items():\n",
    "    print(f\"{model_name:<45} | {mae:>14.4f}\")\n",
    "\n",
    "best_model = min(summary, key=summary.get)\n",
    "print(f\"\\nBest model: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | What it does |\n",
    "|---|---|\n",
    "| `pd.read_csv()` | Loads a CSV file into a DataFrame |\n",
    "| **Series** | A single labeled column of data |\n",
    "| **DataFrame** | A table of data (collection of Series) |\n",
    "| `.describe()` | Summary statistics for numeric columns |\n",
    "| `.value_counts()` | Frequency count of each unique value |\n",
    "| **Decision Tree** | Splits data on feature thresholds to make predictions |\n",
    "| `max_leaf_nodes` | Limits tree depth to control overfitting/underfitting |\n",
    "| **Random Forest** | Ensemble of many trees — generally more accurate and robust |\n",
    "| **MAE** | Average absolute difference between predictions and actual values |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
